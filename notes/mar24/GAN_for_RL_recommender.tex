\documentclass{article}
\title{Notes for: Generative Aversarial User Model for Reinforcement Learning Based Recommendation System}
\author{Michael Nefiodovas. \thanks{paper: https://arxiv.org/pdf/1812.10613.pdf}}
\date{\today}

\begin{document}
    \maketitle

    \section{Setting}

    \subsection{Intuition}
    \begin{itemize}
        \item You work at YouTube and want to recommend YouTube videos to your users to maximise their "utility" from watching videos.
        \item You have access to a set of all available videos (all YouTube videos) to show the user.
        \item You must design an algorithm to select a subset of videos to recommend to the user after they watch a video.
        \item Once shown a selection of recommendations, the user may click on one of the videos to watch or may abstain from clicking.
        \item You have access to users' historical view data.
        \item[Summary:] \emph{users are recommended a page of items and they provide feedback, and then the system recommends a new page of items.}
    \end{itemize}

    \subsection{Assumptions}
    \begin{enumerate}
        \item Users are not random and given a set of $k$ items, users will attempt to maximise their own reward/utility $r$.
        \item Watching each video incurs an opportunity cost, if watching a video is a waste of time or mental energy the user can choose to \emph{not} click on any recommendations.
        \item Reward is the marginal benefit of taking an action, this means that $r$ depends not only on the video about to be watched but also the user's watch history leading up to this video\footnote{Someone might not be interested in Taylor Swift at the beginning but do become interested after listening to it. Users may also get bored if they watch the same video 100 times}.
    \end{enumerate}

    \section{Framing}

    

\end{document}