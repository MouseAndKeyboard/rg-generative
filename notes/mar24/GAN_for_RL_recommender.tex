\documentclass{article}
\title{Notes for: Generative Aversarial User Model for Reinforcement Learning Based Recommendation System}
\author{Michael Nefiodovas. \thanks{paper: https://arxiv.org/pdf/1812.10613.pdf}}
\date{\today}
\usepackage{graphicx}

\begin{document}
    \maketitle

    \section{Setting}

    \subsection{Intuition}
    \begin{itemize}
        \item You work at YouTube and want to recommend YouTube videos to your users to maximise their "utility" from watching videos.
        \item You have access to a set of all available videos (all YouTube videos) to show the user.
        \item You must design an algorithm to select a subset of videos to recommend to the user after they watch a video.
        \item Once shown a selection of recommendations, the user may click on one of the videos to watch or may abstain from clicking.
        \item You have access to users' historical view data.
        \item[Summary:] \emph{users are recommended a page of items and they provide feedback, and then the system recommends a new page of items.}
    \end{itemize}

    \subsection{Assumptions}
    \begin{enumerate}
        \item Users are not random and given a set of $k$ items, users will attempt to maximise their own reward/utility $r$.
        \item Watching each video incurs an opportunity cost, if watching a video is a waste of time or mental energy the user can choose to \emph{not} click on any recommendations.
        \item Reward is the marginal benefit of taking an action, this means that $r$ depends not only on the video about to be watched but also the user's watch history leading up to this video\footnote{Someone might not be interested in Taylor Swift at the beginning but do become interested after listening to it. Users may also get bored if they watch the same video 100 times}.
    \end{enumerate}

    \section{Framing}
    \includegraphics[width=10cm]{fig1.png}
    
    \textbf{Environment}

    It may be strange to think about it at first but:
    The "user" in this scenario is the environment which we interact with by selecting videos/documents to recommend. The user obeys their policy $\phi ^* (S^t, A^t)$, selecting a choice based on the current state and the recommending agent's selected action (what videos were shown).

    \textbf{State}

    The current environment state $s^t$ refers to "an ordered sequenceof a userâ€™s historical clicks".

    The current state can be represented as an embedding of the historical sequence of items clicked by the user.

    \textbf{Action}

    The action is a selection of videos to present the user, that means if there is $\mathcal{I}$ total available videos to select from and you are showing the user $k$ ideos at a time, then action $A^t$ is selected $A^t \in {\mathcal{I^t} \choose k}$. As you can imagine, the possible action space is absolutely huge even for medium sized $I^t$ and $k$\footnote{This problem is addressed through their \emph{Cascading Q-Network} architecture.}. 

    \textbf{Reward}

    $r(s^t, A^t, a^t)$ measures a user's utility or satisfaction after making a choice. A recommendation system should attempt to maximise this. (in theory this reward could also consider company goals to align the recommneder with the company moreso).

    Importantly, measuring a users utility is a hard task\footnote{This problem is addressed through their \emph{Generative Adversarial User Model}} as we do not know $\phi ^*$ and often times "synthetic" reward signals are crafted (e.g. +1 reward whenever you click an item).

    \section{Contribution}

    \subsection{Generative Adversarial User Model}
    We need a way to estimate $\phi ^*$ so that we can accurately reccomend things to the user.

    We have a generator $\phi$ which tries to mimic the action sequences provided by a real user (note: all users attempt to maximise their reward function $r$).

    We have a discriminator $r$ which tries to differentiate the actual user's actions from the generated actions.

    As mentioned before the state can be represented as an embedding of past states. They provide two suggestions on how to to this: via LSTM or via a position weight matrix

    They mention somgthing about how the process "may be unstable due to the non-complexity nature of the problem". I didn't really have time to comprehend what these things mean but they say they use a some sort of "special regularization" for initializing the training process.

    \subsection{Cascading RL Policy for Recommendation}

    One of the problems was the combinatorial action space ${\mathcal{I}^t \choose k}$
    They solve this by creating $k$-many Q functions and applying them in a cascading fashion.
    
    Recomender actions as $A = {a_1 : k} \subset \mathcal{I}$

    Optimal action as $A^* = {a_{1: k}^*} = \arg \max _A Q^* (s, A)$

    $$\max_{a_1 : k} Q^* (s, a_{1 : k}) = \max_{a_1} (\max_{a_2 : k} Q^* (s, a_{1 : k}))$$

    Each Q-function considers the previous selection to decide the next document/video to select/add to the set which will be recommended to the user.

    
    
\end{document}