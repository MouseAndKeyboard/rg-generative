\documentclass{article}
\title{Notes for: Learning to Communicate with Deep Multi-Agent Reinforcement Learning}
\author{Michael N.\thanks{paper: https://arxiv.org/abs/1605.06676}}
\date{\today}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}
    \maketitle
    \section{High Level and Motivation}

    \subsection{Motivation}

    \paragraph{Switch Riddle}
    

    \emph{
        \begin{enumerate}
            \item After months of evading the police, you have finally been caught and sent to prison.
            \item You are put into a cell, unable to communicate with any of your partners in crime.
            \item Each day the warden selects a prisoner at random and brings them to the interrogation room
            \item The interrogation room contains only a light bulb and a toggle switch
            \item The prisoner can observe the current state of the light bulb and can toggle the light bulb ON/OFF
            \item The prisoner may also announce if they believe all the prisoners have visited the interrogation room
            \item If the prisoner's announcement is true, then all prisoners are set free, but if the announcement is incorrect, all the prisoners are executed.
            \item[When they arrive:] The warden leaves and the prisoners huddle together to discuss their fate. Can they agree on a protocol that will guarantee their freedom?
        \end{enumerate}}

        \paragraph{} Can we enable agents to form communication protocols using deep reinforcement learning?

    \subsection{Problem definition}

    \paragraph{Explicitly} 
    How can we apply an end-to-end deep RL approach to learning:
    fully cooperative, partially observable, sequential multi-agent decision making environments.
    \paragraph{Definitions}
    \begin{enumerate}
        \item[multi-agent] There are multiple agents which try to maximise their own reward signals in an environment.
        \item[partially observable] Each agent does not have access to the underlying markov state of the environment, each agent can only make \emph{observations} which are correlated with the true state.
        \item[sequential] There is a temporal/sequential aspect to the environments where observed states are dependent on previous states and actions.
        \item[fully cooperative] Agents' reward signals are shared (each agent has its own reward function however the value we maximise is the sum of rewards for all agents [basically rewards are shared among all agents])
        \item[end-to-end deep RL] A single neural network architecture. (as opposed to multiple neural networks)
    \end{enumerate}
        
    \paragraph{Why partially observable?} There is a strong incentive to communicate in a multi agent environment if the the true state is not fully known to an agent.

    \subsection{Architecture}

    \paragraph{} They discuss two architectures for enabling communication between agents:
    \begin{enumerate}
        \item[RIAL] \emph{Reinforced Inter-Agent Learning}
        \item[DIAL] \emph{Differentiable Inter-Agent Learning}
    \end{enumerate}

    \subsubsection{Reinforced Inter-Agent Learning}

    \paragraph{What is RIAL?} RIAL is the standard way of doing multi-agent reinforcement learning. 
    \begin{enumerate}
        \item One main approach is to basically leave a bunch of RL agents in an environment and they must maximise their own reward, treating other agents as part of the environment.
        \item Another approach is to train a single network which works for all agents and then each agent uses this central network for choosing actions. Then each agent updates the central network based on its reward.
    \end{enumerate}

    \paragraph{} A main conceptual fact of RIAL is that each agent itself is end-to-end trainable (basically you can train each agent independently and their own trainings don't really impact other agents [besides their impact on the environment]) but it is not end-to-end trainable across agents (there are no gradient flows between agents).


    \subsubsection{Differentiable Inter-Agent Learning}

    \paragraph{What is DIAL?} DIAL is the new method proposed in the paper and allows for end-to-end training across agents. 
   
    \paragraph{} In reality it's one big network, however, conceptually it's easier to think about DIAL being lots of smaller networks for each agent with a single connection between each network allowing for communication. Because each network is linked through these connections, backpropagation is able to directly inform networks how to change how they are communicating through this channel.

    \section{Core content}

    \subsection{Notation}
    \begin{enumerate}
        \item[$r_t$] Shared team reward
        \item[$u^a_t \in U$] Environment action
        \item[$m^a_t \in M$] Communication action
        \item[$s_t$] Global (underlying) state
        \item[$o^a_t$] What the agent observes
        \item[$Q(o_t, h_{t-1}, u)$] Q-functon with internal state
    \end{enumerate}

    \subsection{Setting}
    \paragraph{rewards}All agents seek to maximise $R_t$ as reward is shared.
    \paragraph{actions} Each time step $t$, two actions are picked
    \begin{enumerate}
        \item[environment action] Directly impacts the environment (think: traditional action such as moving)
        \item[communicaton action] A "binary" vector which is observed by other agents but has no direct impact on reward: $\begin{bmatrix}0 & 1 & 0 & 0 & 1 & 1\end{bmatrix}^T$
    \end{enumerate}
    \paragraph{states} The environment is partially observable, each agent is able to make observations $o_t^a$ which are correlated with the markov state $s_t$

    \subsection{How to build}

    
    \section{Their Experiments and Comments}
\end{document}