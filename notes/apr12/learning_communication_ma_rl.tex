\documentclass{article}
\title{Notes for: Learning to Communicate with Deep Multi-Agent Reinforcement Learning}
\author{Michael N.\thanks{paper: https://arxiv.org/abs/1605.06676}}
\date{\today}
\usepackage{graphicx}

\begin{document}
    \maketitle
    \section{High Level and Motivation}

    \subsection{Motivation}

    \paragraph{Switch Riddle}
    

    \emph{
        \begin{enumerate}
            \item After months of evading the police, you have finally been caught and sent to prison.
            \item You are put into a cell, unable to communicate with any of your partners in crime.
            \item Each day the warden selects a prisoner at random and brings them to the interrogation room
            \item The interrogation room contains only a light bulb and a toggle switch
            \item The prisoner can observe the current state of the light bulb and can toggle the light bulb ON/OFF
            \item The prisoner may also announce if they believe all the prisoners have visited the interrogation room
            \item If the prisoner's announcement is true, then all prisoners are set free, but if the announcement is incorrect, all the prisoners are executed.
            \item[When they arrive:] The warden leaves and the prisoners huddle together to discuss their fate. Can they agree on a protocol that will guarantee their freedom?
        \end{enumerate}}

        \paragraph{} Can we enable agents to form communication protocols using deep reinforcement learning?

    \subsection{Problem definition}

    \paragraph{Explicitly} 
    How can we apply an end-to-end deep RL approach to learning:
    fully cooperative, partially observable, sequential multi-agent decision making environments.
    \paragraph{Definitions}
    \begin{enumerate}
        \item[multi-agent] There are multiple agents which try to maximise their own reward signals in an environment.
        \item[partially observable] Each agent does not have access to the underlying markov state of the environment, each agent can only make \emph{observations} which are correlated with the true state.
        \item[sequential] There is a temporal/sequential aspect to the environments where observed states are dependent on previous states and actions.
        \item[fully cooperative] Agents' reward signals are shared (each agent has its own reward function however the value we maximise is the sum of rewards for all agents [basically rewards are shared among all agents])
        \item[end-to-end deep RL] A single neural network architecture. (as opposed to multiple neural networks)
    \end{enumerate}
        
    \subsection{Architecture}

    \paragraph{} They discuss two architectures for enabling communication between agents:
    \begin{enumerate}
        \item[RIAL] \emph{Reinforced Inter-Agent Learning}
        \item[DIAL] \emph{Differentiable Inter-Agent Learning}
    \end{enumerate}

    \subsubsection{Reinforced Inter-Agent Learning}

    \paragraph{What is RIAL?} RIAL is the standard way of doing multi-agent reinforcement learning. 
    \begin{enumerate}
        \item One main approach is to basically leave a bunch of RL agents in an environment and they must maximise their own reward, treating other agents as part of the environment.
        \item Another approach is to train a single network which works for all agents and then each agent uses this central network for choosing actions
    \end{enumerate}

    \paragraph{} A main conceptual fact of RIAL is that each agent itself is end-to-end trainable (basically you can train each agent independently and their own trainings don't really impact other agents [besides their impact on the environment]) but it is not end-to-end trainable across agents (there are no gradient flows between agents).


    \subsubsection{Differentiable Inter-Agent Learning}

    \paragraph{What is DIAL?} DIAL is the new method proposed in the paper and allows for end-to-end training across agents. 
   


    \section{Core content}

    
    



    \section{Their Experiments and Comments}
\end{document}