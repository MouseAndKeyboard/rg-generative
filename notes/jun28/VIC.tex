\documentclass{article}
\title{Notes for: Variational Intrinsic Control}
\author{M. Nef. \thanks{paper: https://arxiv.org/abs/1611.07507}}
\date{\today}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{calc}
\usepackage{graphicx}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle

\section{Preliminaries}

\subsection{What are ``options''}

An option is a procedure which can be executed by an agent. Once an option has been executed it will take actions for the agent and eventually, at some point, return control back to the agent. In other words: It's some policy with a termination condition.


An option itself is not a policy, an option, \(\in \Omega\) (all possible options), is just some element in the \(\Omega\) space.
A policy for a particular option is defined as:
\begin{equation}
  \label{eq:1}
  \pi(a | s, \Omega)
\end{equation}

This comes to be important, since, depending on how we define \(\Omega\) we will be able to look at how similar different options are.


When an option's policy is executed, the current state the agent is initially in is denoted \(s_{0}\). The final state the agent is in when the option's policy returns control is denoted \(s_{f}\).


In this paper, they say two intrinsic options are identical if when both starting in \(s_{0}\) they both reach the same terminal state \(s_{f}\). Although not explicitly written I assume an extrinsic option does not have this quality. Intrinsic options can therefore be described as options which meaningfully affect the world.


\begin{figure}[!ht]
\begin{center}
  \includegraphics[width=60mm]{images/same_options.png}
  \caption{These two intrinsic options which are considered the same. both start at (1,1) and terminate at (4,4) however their path is different.}
\end{center}
\end{figure}

\pagebreak
Additionally, there are 2 types of options. {\bf Closed loop} options, and {\bf Open loop} options.
\paragraph{Open Loop} options are where you select a sequence of actions to execute and the agent then blindly follows this pre-decided sequence (regardless of new observations).
\paragraph{Closed Loop} options are where you run a policy conditioned on the option representation. This allows the agent to respond to changes in the environment.

\section{Body}

They create an algorithm to generate as many {\bf different options} for an agent as possible. Each of these options should terminate at a different location to all of the others. These options can then be executed by the agent to either maximise reward or maximise ``empowerment'' (which is basically a measure of an agent's ability to reach new states from the current state).


\paragraph{What's so special?} Normally, options are relatively rigid and are simply policies which are learnt by the agent over time, they appear to the agent as actions and return control once they terminate. This paper wants a way to build up a set of options which will allow the agent to get to, or get close to, any state it desires. The point of this paper isn't to make up options which will maximise reward, simply to make up options which will let the agent ``get around''.


\paragraph{What's this about empowerment!?} Using empowerment as an objetive, the agent will seek out states which have the highest number of intrinsic options. Importantly, the agent should be good at actually executing these option policies (otherwise what's the point). Because this is the reinforcement learning paradigm, the agent initially has no understanding of the options which could be available to it, therefore it still needs to explore the environment to find these high empowerment states.


\paragraph{Relationship with unsupervised learning.} In standard UL we have the data likelihood metric for figuring out how good our unsupervised learning algorithm is, we have some data, our UL algorithm identifies some structure and we look at the likelihood to see how well the model explains the data. In the context of this paper we want to do some sort of unsupervised learning to find the intrinsic options available to the agent.


The difference in this case is that with RL, not all the data is known beforehand, the agent needs to interact with the environment to expose examples of different states and environment dynamics.


In RL, instead of using likelihood, the empowerment metric can be used. A higher empowerment suggests our model of the different options is good, since it's basically saying ``our options, with high confidence, will lead to specific final states''.

\begin{equation}
  \label{eq:2}
  \text{Empowerment measures the mutual information between action choices and final states}
\end{equation}

\section{Intrinsic Control}

\paragraph{How does an options policy work?} In practice, the policy \(\pi(a|s,\Omega)\) acts as a normal policy, selecting actions based on states. Additionally, this policy has a special termination action which ends the options policy and returns control to the ``master policy''. The \(\Omega\) this options policy is conditioned on can be constructed in a variety of ways:

\begin{enumerate}
  \item Each \(\Omega\) takes a finite \(n\) number of values. Where \(\forall \; \Omega_{i} \implies \pi_{i}\).
  \item \(\Omega\) is a binary vector of length \(n\) therefore there is \(2^{n}\) different possible options.
    \item \(\Omega \in \Re^{d}\) is a \(d\)-dimensional vector. Policies for nearby \(\Omega\)s will be similar.
\end{enumerate}

\paragraph{Intrinsic Options} Intrinsic options are a subset of all possible options. Two intrinsic options are the same where \(s_{0}\) maps to the same final state \(s_{f}\) following that option. If our environment is stochastic then even the same option may lead to slightly different states. Therefore an option defines this distribution.
\begin{equation}
  \label{eq:3}
  p^{J}(s_{f}|s_{0}, \Omega)
\end{equation}

Since an intrinsic option is more an ``idea'' than an actual option (it's the idea of going from point A to point B), we want a fair way to select different intrinsic options - not really caring about how it gets done.
If we were to uniformly randomly select an intrinsic option we would have to have a way of grouping options which complete the same intrinsic option together. This is best explained with an example.


If we have 3 options, \(\Omega_{1}, \Omega_{2}, \Omega_{3}\) and \(\Omega_{1}\) always leads to \(s_{1}\) while \(\Omega_{2} \text{ and } \Omega_{3}\) both lead to \(s_{2}\) then, to maximise behaviour diversity (do the most stuff), half the time we would select \(\Omega_{1}\) and half the time we would select (not the first option), we would randomly pick between \(\Omega_{2}\) and \(\Omega_{3}\). The probability of selecting any particular \(\Omega_{k}\) is given by the following, this is the ``controllability distribution'':
\begin{equation}
  \label{eq:4}
  p^{C}(\Omega | s_{0})
  \end{equation}


  Using definitions \eqref{eq:3} and \eqref{eq:4} it's straightforward to see:
\begin{equation}
  \label{eq:5}
  p(s_{f}|s_{0}) = \sum_{\Omega} p^{J}(s_{f}|s_{0},\Omega) p^{C}(\Omega | s_{0})
\end{equation}
  probability of ending up in a state conditioned on an option multiplied by the probability of said option actually being selected.


  Now, since we want to get a diverse range of possible intrinsic options, we want to maximise the entropy of the final state \(s_{f}\).

  \paragraph{Objective} Maximising the mutual information \(I(\Omega, s_{f}| s_{0})\) between options and final states under the distribution \(p(\Omega,s_{f}|s_{0}) = p^{J}(s_{f}|s_{0}, \Omega)p^{C}(\Omega | s_{0})\).


  If we have 2 options \(\Omega_{1}\) and \(\Omega_{2}\) and we're currently in state \(s_{f_{1}}\), if we can correctly say that it was \(\Omega_{1}\) which must've just been executed and then in some other terminal state \(s_{f_{2}}\) we can correctly say that \(\Omega_{2}\) was selected, then these are intrinsically different options.



  Because we want a large set of intrinsic options, we want to maximise the entropy of \eqref{eq:4} which is given by:

\begin{equation}
  \label{eq:6}
-\sum_{\Omega} p^{C}(\Omega|s_{0}) \log p^{C}(\Omega | s_{0})
\end{equation}
Remember, this just means picking options which are the most different, and this is done by ``grouping'' similar options together.

We don't just want to maximise this entropy since an option which just randomly moves around is kind of useless however this option could potentially be included if we only maximised the entropy here.


This is why we must also maximise \(\log p(\Omega | s_{0}, s_{f})\). In words, we want to be confident that a specific option was selected based on the start and end point. You can see, an option which randomly moves around is going to be filtered out here since there's a really low chance of it being able to consistently transition from \(s_{0} \to s_{f}\).

\begin{equation}
  \label{eq:7}
  q(\Omega | s_{0}, s_{f})
\end{equation}

q predicts which option was selected based on the start and end points.
\pagebreak
\section{The actual algorithm}

The following functions are approximated by neural networks (or similar):
\begin{enumerate}
  \item[\(p^{C}(\Omega | s_{0})\)] Probability of selecting this option initially.
  \item[\(q(\Omega | s_{0} , s_{f})\)] Probability this option was selected based on what we've seen.
  \item[\( \pi (a | \Omega, s )\)] Option's policy
\end{enumerate}


\begin{figure}[!ht]
  \includegraphics[width=190mm]{images/algorithm.png}
  \caption{The main algorithm}
\end{figure}

What it's saying:
We pick an option, \(\Omega\) from the initial distribution then we want to take actions that lead to state \(s_{f}\) so that we can be the most confident that it was indeed the \(\Omega\) option which was picked. If we confidently predict correctly (\(q\) is correct) then other options don't lead to this state (or at least less than this one).

\end{document}
