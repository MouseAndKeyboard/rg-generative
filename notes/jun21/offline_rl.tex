

\documentclass{article}
\title{Notes for: Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}
\author{M. Nef. \thanks{paper: https://arxiv.org/pdf/1812.10613.pdf}}
\date{\today}
\usepackage{amsmath}
\usepackage{amsfonts}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle

\section{Types of Reinforcement Learning}
\paragraph{Online Reinforcement Learning}
Reinforcement learning approach whereby an agent acts into the environment and directly updates the policy based solely on actions from said policy. Trajectories are also trained on in the same order they are collected.

The policy, \(\pi_k\), is updated with streaming data collected by \(\pi_k\) itself.

There is no buffer in online reinforcement learning, unlike Off-Policy Reinforcement Learning.

\paragraph{Off-Policy Reinforcement Learning} The agent's experience is appended to a data buffer (replay buffer), \(\mathcal{D}\), and each new policy, \(\pi_k\) adds additional data to \(\mathcal{D}\).
\(\mathcal{D}\) will therefore comprise samples from \( \pi_0 , \pi_1 , \dots, \pi_k\) and this data will be used to train a new policy \(\pi_{k+1}\).

\paragraph{Offline Reinforcement Learning}
Reinforcement learning algorithms that utilize previously collected data, without additional online data collection.

Offline RL has some dataset, \(\mathcal{D}\), collected by potentially unknown policies (or policy) \(\pi_\beta\).
The training process doesn't interact with the MDP at all.

\section{General RL}
\pagebreak
\paragraph{Policy Gradients}

Since the following is the RL objective (what we want to maximise):

\begin{equation}
\label{eq:1}
J(\pi) = \mathbb{E}_{\tau \sim p_\pi (\tau)} \left[ \sum_{t=0}^H \gamma^t r(s_t, a_t) \right ]
\end{equation}

where: \(r(s_t, a_t)\) is the reward function of the MDP. \(\gamma^t r(s_t , a_t)\) is the discounted reward. \(\ \sum_{t=0}^H \gamma^t r(s_t, a_t) \) is the return conditioned on some selected trajectory. \(\tau \sim p_\pi(\tau)\)
\(\tau\) is a random variable which samples from the Trajectory Distribution. \(\mathbb{E}\) We're looking at an expectation, so we're interested in the discounted future reward of each trajectory weighted by the probability of it occurring.

\begin{equation}
\label{eq:2}
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim p_{\pi_\theta} (\tau)} \left [ \sum_{t=1}^H \gamma^t \nabla_\theta \log \pi_\theta (a_t | s_t) \left( \sum_{t' = t}^H \gamma^{t'-t} r(s_{t'}, a_{t'}) - b(s_t) \right) \right]
\end{equation}

where: \(\theta\) are the parameters of a policy (e.g. NN weights). \( \left( \sum_{t' = t}^H \gamma^{t'-t} r(s_{t'}, a_{t'}) - b(s_t) \right) \) is the return estimator (often denoted \(\hat{A}(s_{t}, a_{t})\)). \(b(s_{t})\) is the ``baseline'' and can be estimated as the average reward over the sampled trajectories.

\paragraph{Approximate Dynamic Programming}

If you can accurately estimate a state or state-action value function (\(V\) and \(Q\) respectively) then we can recover a near optimal policy.

We can express the policy implicitly in terms of the \(Q\)-function:

\begin{equation}
  \label{eq:3}
  \pi(a_{t}, s_{t}) = \delta(a_{t} = \argmax Q (s_{t}, a_{t}) )
\end{equation}
\(\delta\) means deterministic policy.
The idea of Q-learning is to simply get a good estimate for \(Q\) and then use this identity.


\paragraph{Actor-Critic Algorithms}

Combines ideas of policy gradients and ADP. This method uses both a parameterised policy and value function. Often they share parameters (but this is not important).
The point of these methods is to get a better estimate of \(\hat{A} (s,a)\) when doing policy gradient estimation.
Actor critics come in different flavours, you can have on policy (directly estimating \(V^{\pi}(s)\)) or off policy actor critics (estimate \(Q^{\pi}(s,a)\) via parameterised \(Q^{\pi}_{\phi}\)).


W.r.t Offline RL, the off policy actor critics can be extended to an offline setting.


Actor Critics vs Q-learning: In Q-Learning, our goal is to find the BEST \(Q\), \(Q^{*}\), whereas in actor critic we're learning the Q-function for the current policy (so our return estimator is more accurate).

\paragraph{Policy Iteration (PI)}
Related to actor-critic.
Two phases in PI:
\begin{enumerate}
  \item policy evaluation: Compute the \(Q\)-function for \(\pi\)
  \item policy iteration: Actions are selected greedily according the \(Q\)-function.
\end{enumerate}
Monotonic improvements of the policy and converges to optimal policy.

\paragraph{Model-Based Reinforcement Learning}
General term that refers to approaches that estimate the dynamics/transition function \(T_{\psi}(s_{t+1}|s_{t},a_{t})\) [parameters \(\psi\)].

Usually this is done so: planning can be done, backprop through time. It's also used to generate samples to augment ``model-free'' RL leanring methods

\end{document}
