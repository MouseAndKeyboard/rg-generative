

\documentclass{article}
\title{Notes for: Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}
\author{M. Nef. \thanks{paper: https://arxiv.org/pdf/1812.10613.pdf}}
\date{\today}
\usepackage{amsmath}
\usepackage{amsfonts}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle

\section{Types of Reinforcement Learning}
\paragraph{Online Reinforcement Learning}
Reinforcement learning approach whereby an agent acts into the environment and directly updates the policy based solely on actions from said policy. Trajectories are also trained on in the same order they are collected.

The policy, \(\pi_k\), is updated with streaming data collected by \(\pi_k\) itself.

There is no buffer in online reinforcement learning, unlike Off-Policy Reinforcement Learning.

\paragraph{Off-Policy Reinforcement Learning} The agent's experience is appended to a data buffer (replay buffer), \(\mathcal{D}\), and each new policy, \(\pi_k\) adds additional data to \(\mathcal{D}\).
\(\mathcal{D}\) will therefore comprise samples from \( \pi_0 , \pi_1 , \dots, \pi_k\) and this data will be used to train a new policy \(\pi_{k+1}\).

\paragraph{Offline Reinforcement Learning}
Reinforcement learning algorithms that utilize previously collected data, without additional online data collection.

Offline RL has some dataset, \(\mathcal{D}\), collected by potentially unknown policies (or policy) \(\pi_\beta\).
The training process doesn't interact with the MDP at all.

\section{General RL}
\pagebreak
\paragraph{Policy Gradients}

Since the following is the RL objective (what we want to maximise):

\begin{equation}
\label{eq:1}
J(\pi) = \mathbb{E}_{\tau \sim p_\pi (\tau)} \left[ \sum_{t=0}^H \gamma^t r(s_t, a_t) \right ]
\end{equation}

where: \(r(s_t, a_t)\) is the reward function of the MDP. \(\gamma^t r(s_t , a_t)\) is the discounted reward. \(\ \sum_{t=0}^H \gamma^t r(s_t, a_t) \) is the return conditioned on some selected trajectory. \(\tau \sim p_\pi(\tau)\)
\(\tau\) is a random variable which samples from the Trajectory Distribution. \(\mathbb{E}\) We're looking at an expectation, so we're interested in the discounted future reward of each trajectory weighted by the probability of it occurring.

\begin{equation}
\label{eq:2}
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim p_{\pi_\theta} (\tau)} \left [ \sum_{t=1}^H \gamma^t \nabla_\theta \log \pi_\theta (a_t | s_t) \left( \sum_{t' = t}^H \gamma^{t'-t} r(s_{t'}, a_{t'}) - b(s_t) \right) \right]
\end{equation}

where: \(\theta\) are the parameters of a policy (e.g. NN weights). \( \left( \sum_{t' = t}^H \gamma^{t'-t} r(s_{t'}, a_{t'}) - b(s_t) \right) \) is the return estimator (often denoted \(\hat{A}(s_{t}, a_{t})\)). \(b(s_{t})\) is the ``baseline'' and can be estimated as the average reward over the sampled trajectories.

\paragraph{Approximate Dynamic Programming}

If you can accurately estimate a state or state-action value function (\(V\) and \(Q\) respectively) then we can recover a near optimal policy.

We can express the policy implicitly in terms of the \(Q\)-function:

\begin{equation}
  \label{eq:3}
  \pi(a_{t}, s_{t}) = \delta(a_{t} = \argmax Q (s_{t}, a_{t}) )
\end{equation}
\(\delta\) means deterministic policy.
The idea of Q-learning is to simply get a good estimate for \(Q\) and then use this identity.


\paragraph{Actor-Critic Algorithms}

Combines ideas of policy gradients and ADP. This method uses both a parameterised policy and value function. Often they share parameters (but this is not important).
The point of these methods is to get a better estimate of \(\hat{A} (s,a)\) when doing policy gradient estimation.
Actor critics come in different flavours, you can have on policy (directly estimating \(V^{\pi}(s)\)) or off policy actor critics (estimate \(Q^{\pi}(s,a)\) via parameterised \(Q^{\pi}_{\phi}\)).


W.r.t Offline RL, the off policy actor critics can be extended to an offline setting.


Actor Critics vs Q-learning: In Q-Learning, our goal is to find the BEST \(Q\), \(Q^{*}\), whereas in actor critic we're learning the Q-function for the current policy (so our return estimator is more accurate).

\paragraph{Policy Iteration (PI)}
Related to actor-critic.
Two phases in PI:
\begin{enumerate}
  \item policy evaluation: Compute the \(Q\)-function for \(\pi\)
  \item policy iteration: Actions are selected greedily according the \(Q\)-function.
\end{enumerate}
Monotonic improvements of the policy and converges to optimal policy.

\paragraph{Model-Based Reinforcement Learning}
General term that refers to approaches that estimate the dynamics/transition function \(T_{\psi}(s_{t+1}|s_{t},a_{t})\) [parameters \(\psi\)].

Usually this is done so: planning can be done, backprop through time. It's also used to generate samples to augment ``model-free'' RL leanring methods

\section{Offline RL}
We're still trying to maximise (\ref{eq:1}) but we don't have the ability to go and interact with the environment to collect more transitions.


We're provided with a static dataset of transitions \(\mathcal{D} = \{ (s_{t}^{i}, a_{t}^{i},s_{t+1}^{i},r_{t}^{i}) \}\)

This can be called the training set (like in supervised learning).


\(\pi_{\beta}\) is the distribution over states and actions in \(\mathcal{D}\). Where \( (s,a) \in \mathcal{D} \) sampled according to \(s \sim d^{\pi_{\beta}}(s)\) - the overall state visitation frequency averaged over timesteps (i.e. average state visitation frequency). \(a \sim \pi_{\beta} (a | s)\).

\paragraph{disambiguations}
``Off-policy RL'' refers to any algorithm which uses datasets of transitions where the actions in the dataset are not from the current policy.


Off policy RL may still interact with the environment (online data collection).


Offline means that there is no interaction with the environment.
(Therefore offline RL is always Off-policy, but off-policy RL is not necessarily offline).


The consequence of this is that all off-policy RL algorithms can be done technically done in an offline setting (though they're unlikely to yield great results)



\subsection{Challenges}
\paragraph{No exploration} We can't do exploration in offline RL, so if \(\mathcal{D}\) doesn't contain any high reward transitions then we're basically screwed (there could be interesting ideas combining offline RL to pre-train online algorithms).
\paragraph{Counterfactual Reasoning} Since we want to train an agent that performs better than the examples in \(\mathcal{D}\) we need to reason about what the samples could've done better.


This makes it different from standard supervised learning, where the objective usually is to best fit the training data, whereas in RL we don't want to fit the training data, we want to maximise our return.

\paragraph{Distributional Shift} Highly related to counterfactual reasoning, but, our examples are not going to be exactly the same as the ones the agent performs in. i.e. the learned policy, \( \pi(s,a) \) could potentially enter states outside the training distribution, since it's highly likely that \(d^{\pi} \ne d^{\pi_{\beta}}\) (e.g. stochastic environments, insufficient training data, etc). This is mainly an issue in behavioural cloning.

\section{Offline Evaluation Via Importance Sampling}

\paragraph{Off-Policy Evaluation via Importance Sampling} We can use importance sampling to estimate \(J(\pi_{\theta})\) with trajectories sampled from \(\pi_{\beta} (\tau)\). This then allows us to pick the \(\pi_{\theta}\) which maximises \(J(\pi_{\theta})\).


Normally, to find \(J(\pi_{\theta})\) you need actual \(\tau \sim \pi_{\theta} \) but importance sampling lets us use \(\tau \sim \pi_{\beta}\) \footnote{\(\pi(\tau) = \prod_{t=0}^{H} \pi(a_{t}, s_{t}) \)}.

\begin{equation}
\label{eq:4}
  J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\beta}(\tau)} \left [ \frac{\pi_{\theta}(\tau)}{\pi_{\beta}(\tau)} \sum_{t=0}^{H}\gamma^{t}r(s,a) \right ]
\end{equation}

\paragraph{Off-Policy Policy Gradient} Basically using the same method as before (importance sampling), we can adapt the policy gradient (2) when sampling only from \(\pi_{\beta}\) rather than \(\pi_{\theta}\).

\begin{equation}
  \label{eq:5}
  \nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{beta}(\tau)} \left [ \frac{\pi_{\theta}(\tau)}{\pi_{\beta}(\tau)} \sum_{\pi_{\beta}(\tau)}^{H} \gamma ^{t} \nabla \log \pi_{\theta}(a_{t}|s_{t})\hat{A}(s_{t},a_{t})  \right]
  \end{equation}

  \paragraph{Marginalised Importance Sampling} Remember that \(d^{\pi}(s)\) is a distribution over state visitation frequencies.

  The \emph{state-marginal importance ratio} \(\rho^{\pi_{\theta}}= \frac{d^{\pi_{\theta}}(s)}{d^{\pi_{\beta}}(s)} \)


  I didn't fully understand this part and didn't have time to look into it.

\subsection{Challenges and Comments}
\begin{enumerate}
  \item Many of these methods have been done in online off-policy RL. There hasn't been much research into offline usage of these algorithms.
  \item Importance sampling suffers from high variance. As \(\pi_{\theta}\) drifts from \(\pi_{\beta}\) the importance weights become useless. In offline RL you have no way to sample more data to decrease the distance.
\end{enumerate}
\pagebreak
\section{Distributional Shift in Offline Reinforcement Learning via Dynamic Programming}

\paragraph{Distributional Shift in Offline Reinforcement Learning via Dynamic Programming} Since the offline data collected comes from an unknown behavioural policy, \(\pi_{\beta}(a|s)\) with state visitation frequency \(d^{\pi_{\beta}}(s)\) when we actually run our RL agent in test it will likely have a \(d^{\pi} \ne d^{\pi_{\beta}}\), AKA distributional shift.


The effects of distributional shift can be minimised by limiting how much we let \(\pi\) diverge from \(\pi_{\beta}\). In practice this means adding a constraint so that the KL divergence between the two is smaller than some bound. This kind of sucks because it means we can't really do much better than \(p_{\beta}\) since we're constrained to be close to it.

\paragraph{Policy Constraint Methods}
These are different methods to attempt to hold the learned policy, \(\pi\), close to the behavioural policy, \(\pi_{\beta}\) when learning the value function (or Q function).
\begin{enumerate}
  \item explicit f-divergence constraints
  \item implicit f-divergence constraints
  \item integral probability metric (IPM)
\end{enumerate}

\paragraph{Offline Approximate Dynamic Programming with Uncertainty Estimation} Constraints can make the Q function unlikely to suggest out-of-distribution actions. We can measure the epistemic uncertainty of the Q-function. It's fair to say that there will be a higher epistemic uncertainty about out of distribution actions and therefore we can adjust the policy to avoid these actions more often.


This is done by learning an uncertainty distribution over many possible Q-functions from the data \(\mathcal{D}\)

\subsection{Challenges and Open Problems}
\begin{enumerate}
  \item Distributional shift makes ADP algorithms perform very poorly in offline situations.
  \item Uncertainty estimation seems ideal, however it's problematic since it's very hard to get high accuracy uncertainty estimates. If the estimate is incorrect and overly conservative it can prevent learning, and in the other direction it can fail to prevent OOD actions.
  \item Since most policy constraint methods first estimate a model of the behaviour policy \(\pi_{\beta}(\cdot | s)\) to constrain the learned policy against, the effectiveness of this is highly dependent on the ability to accurately estimate \(\pi_{\beta}\).
    \end{enumerate}

\section{Offline Model-Based Reinforcement Learning}
Learning a model is highly beneficial in offline RL since it can be solved using normal supervised learning approaches. It doesn't need importance sampling, or any other variance-altering methods. This is because the environment dynamics are assumed to be the same during test time.

\paragraph{Model Exploitation and Distribution Shift}
Models are usually used for either: Planning (directly using the model) or training a policy (using the model to make a policy perform better).


The states that a dynamics model will have to process during test is determined by the state visitation frequency: \(s \sim d^{\pi}(s)\). Here, \(\pi\) could either be a normal, explicit, policy or it could be an ``implicit'' policy (running a planning algorithm).


Since the model has to predict based on \(d^{\pi}\) it faces distribution shift from both states and actions (if the actions aren't the same as the behaviour policy's then it won't know what next-state to predict, if the agent is currently in an OOD state it will also be hard for the world model to know the next state).

\paragraph{Model Exploitation} Since the world model is susceptible to distribution shift, if OOD actions are taken or OOD states are entered it's very possible for the model to erroneously predict a state which the policy would highly value. This can lead to the policy intentionally selecting OOD actions to make the model screw up and predict great next states (even though the next state won't actually be any good).


Similar to earlier, the epistemic uncertainty of the model can be estimated of the learned transition model \(T_{\phi} (s_{t+1}| s_{t},s_{a})\). Bayesian neural networks (I haven't learned about these yet) can be used for this epistemic uncertainty estimation.


\paragraph{Model-Based Offline Reinforcement Learning} As long as the model exploitation is controlled, it's totally acceptable for the model to be similar to what we observe in the training set (since this is typical SL). In general, models tend to be very good in offline RL.

\begin{enumerate}
  \item Action-conditioned convolutional neural networks have been very successful in Atari games.
  \item Prediction of future rewards or ``reward features'' conditioned on a sequence of future actions (with no actual prediction of observations).
  \item Special constraints (which cause ``Lyapunov stability'' - need to look into) on the model can cause the policy to be constrained to only take ``safe'' actions.
  \item Deep imitatative models (DIMs) learn a normalising flow model over future trajectories.
\end{enumerate}

\subsection{Challenges and Open Problems}

\begin{enumerate}
  \item Although model based RL works well, most of the research has been in online, off-policy RL and then basically slapping it onto offline Rl (not much focused direct research).
  \item Long time horizon prediction and very-high dimensional observations are hard to predict
    \item It hasn't been shown theoretically that model-based RL can/should improve over model-free DP algorithms. They both solve a prediction problem (DP predicts future reward and Models predict future state). (this is an open question, although not really  - since empirically models are very useful)
    \end{enumerate}

\end{document}
