

\documentclass{article}
\title{Notes for: Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}
\author{M. Nef. \thanks{paper: https://arxiv.org/pdf/1812.10613.pdf}}
\date{\today}
\usepackage{amsmath}
\usepackage{amsfonts}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
\maketitle

\section{Types of Reinforcement Learning}
\paragraph{Online Reinforcement Learning}
Reinforcement learning approach whereby an agent acts into the environment and directly updates the policy based solely on actions from said policy. Trajectories are also trained on in the same order they are collected.

The policy, \(\pi_k\), is updated with streaming data collected by \(\pi_k\) itself.

There is no buffer in online reinforcement learning, unlike Off-Policy Reinforcement Learning.

\paragraph{Off-Policy Reinforcement Learning} The agent's experience is appended to a data buffer (replay buffer), \(\mathcal{D}\), and each new policy, \(\pi_k\) adds additional data to \(\mathcal{D}\).
\(\mathcal{D}\) will therefore comprise samples from \( \pi_0 , \pi_1 , \dots, \pi_k\) and this data will be used to train a new policy \(\pi_{k+1}\).

\paragraph{Offline Reinforcement Learning}
Reinforcement learning algorithms that utilize previously collected data, without additional online data collection.

Offline RL has some dataset, \(\mathcal{D}\), collected by potentially unknown policies (or policy) \(\pi_\beta\).
The training process doesn't interact with the MDP at all.

\section{General RL}
\pagebreak
\paragraph{Policy Gradients}

Since the following is the RL objective (what we want to maximise):

\begin{equation}
\label{eq:1}
J(\pi) = \mathbb{E}_{\tau \sim p_\pi (\tau)} \left[ \sum_{t=0}^H \gamma^t r(s_t, a_t) \right ]
\end{equation}

where: \(r(s_t, a_t)\) is the reward function of the MDP. \(\gamma^t r(s_t , a_t)\) is the discounted reward. \(\ \sum_{t=0}^H \gamma^t r(s_t, a_t) \) is the return conditioned on some selected trajectory. \(\tau \sim p_\pi(\tau)\)
\(\tau\) is a random variable which samples from the Trajectory Distribution. \(\mathbb{E}\) We're looking at an expectation, so we're interested in the discounted future reward of each trajectory weighted by the probability of it occurring.

\begin{equation}
\label{eq:2}
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim p_{\pi_\theta} (\tau)} \left [ \sum_{t=1}^H \gamma^t \nabla_\theta \log \pi_\theta (a_t | s_t) \left( \sum_{t' = t}^H \gamma^{t'-t} r(s_{t'}, a_{t'}) - b(s_t) \right) \right]
\end{equation}

where: \(\theta\) are the parameters of a policy (e.g. NN weights). \( \left( \sum_{t' = t}^H \gamma^{t'-t} r(s_{t'}, a_{t'}) - b(s_t) \right) \) is the return estimator (often denoted \(\hat{A}(s_{t}, a_{t})\)). \(b(s_{t})\) is the ``baseline'' and can be estimated as the average reward over the sampled trajectories.

\paragraph{Approximate Dynamic Programming}

If you can accurately estimate a state or state-action value function (\(V\) and \(Q\) respectively) then we can recover a near optimal policy.

We can express the policy implicitly in terms of the \(Q\)-function:

\begin{equation}
  \label{eq:3}
  \pi(a_{t}, s_{t}) = \delta(a_{t} = \argmax Q (s_{t}, a_{t}) )
\end{equation}
\(\delta\) means deterministic policy.
The idea of Q-learning is to simply get a good estimate for \(Q\) and then use this identity.


\paragraph{Actor-Critic Algorithms}

Combines ideas of policy gradients and ADP. This method uses both a parameterised policy and value function. Often they share parameters (but this is not important).
The point of these methods is to get a better estimate of \(\hat{A} (s,a)\) when doing policy gradient estimation.
Actor critics come in different flavours, you can have on policy (directly estimating \(V^{\pi}(s)\)) or off policy actor critics (estimate \(Q^{\pi}(s,a)\) via parameterised \(Q^{\pi}_{\phi}\)).


W.r.t Offline RL, the off policy actor critics can be extended to an offline setting.


Actor Critics vs Q-learning: In Q-Learning, our goal is to find the BEST \(Q\), \(Q^{*}\), whereas in actor critic we're learning the Q-function for the current policy (so our return estimator is more accurate).

\paragraph{Policy Iteration (PI)}
Related to actor-critic.
Two phases in PI:
\begin{enumerate}
  \item policy evaluation: Compute the \(Q\)-function for \(\pi\)
  \item policy iteration: Actions are selected greedily according the \(Q\)-function.
\end{enumerate}
Monotonic improvements of the policy and converges to optimal policy.

\paragraph{Model-Based Reinforcement Learning}
General term that refers to approaches that estimate the dynamics/transition function \(T_{\psi}(s_{t+1}|s_{t},a_{t})\) [parameters \(\psi\)].

Usually this is done so: planning can be done, backprop through time. It's also used to generate samples to augment ``model-free'' RL leanring methods

\section{Offline RL}
We're still trying to maximise (\ref{eq:1}) but we don't have the ability to go and interact with the environment to collect more transitions.


We're provided with a static dataset of transitions \(\mathcal{D} = \{ (s_{t}^{i}, a_{t}^{i},s_{t+1}^{i},r_{t}^{i}) \}\)

This can be called the training set (like in supervised learning).


\(\pi_{\beta}\) is the distribution over states and actions in \(\mathcal{D}\). Where \( (s,a) \in \mathcal{D} \) sampled according to \(s \sim d^{\pi_{\beta}}(s)\) - the overall state visitation frequency averaged over timesteps (i.e. average state visitation frequency). \(a \sim \pi_{\beta} (a | s)\).

\paragraph{disambiguations}
``Off-policy RL'' refers to any algorithm which uses datasets of transitions where the actions in the dataset are not from the current policy.


Off policy RL may still interact with the environment (online data collection).


Offline means that there is no interaction with the environment.
(Therefore offline RL is always Off-policy, but off-policy RL is not necessarily offline).


The consequence of this is that all off-policy RL algorithms can be done technically done in an offline setting (though they're unlikely to yield great results)



\subsection{Challenges}
\paragraph{No exploration} We can't do exploration in offline RL, so if \(\mathcal{D}\) doesn't contain any high reward transitions then we're basically screwed (there could be interesting ideas combining offline RL to pre-train online algorithms).
\paragraph{Counterfactual Reasoning} Since we want to train an agent that performs better than the examples in \(\mathcal{D}\) we need to reason about what the samples could've done better.


This makes it different from standard supervised learning, where the objective usually is to best fit the training data, whereas in RL we don't want to fit the training data, we want to maximise our return.

\paragraph{Distributional Shift} Highly related to counterfactual reasoning, but, our examples are not going to be exactly the same as the ones the agent performs in. i.e. the learned policy, \( \pi(s,a) \) could potentially enter states outside the training distribution, since it's highly likely that \(d^{\pi} \ne d^{\pi_{\beta}}\) (e.g. stochastic environments, insufficient training data, etc). This is mainly an issue in behavioural cloning.

\section{Offline Evaluation Via Importance Sampling}

\paragraph{Off-Policy Evaluation via Importance Sampling} We can use importance sampling to estimate \(J(\pi_{\theta})\) with trajectories sampled from \(\pi_{\beta} (\tau)\). This then allows us to pick the \(\pi_{\theta}\) which maximises \(J(\pi_{\theta})\).


Normally, to find \(J(\pi_{\theta})\) you need actual \(\tau \sim \pi_{\theta} \) but importance sampling lets us use \(\tau \sim \pi_{\beta}\) \footnote{\(\pi(\tau) = \prod_{t=0}^{H} \pi(a_{t}, s_{t}) \)}.

\begin{equation}
\label{eq:4}
  J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{\beta}(\tau)} \left [ \frac{\pi_{\theta}(\tau)}{\pi_{\beta}(\tau)} \sum_{t=0}^{H}\gamma^{t}r(s,a) \right ]
\end{equation}

\paragraph{Off-Policy Policy Gradient} Basically using the same method as before (importance sampling), we can adapt the policy gradient (2) when sampling only from \(\pi_{\beta}\) rather than \(\pi_{\theta}\).

\begin{equation}
  \label{eq:5}
  \nabla_{\theta} J(\pi_{\theta}) = \mathbb{E}_{\tau \sim \pi_{beta}(\tau)} \left [ \frac{\pi_{\theta}(\tau)}{\pi_{\beta}(\tau)} \sum_{\pi_{\beta}(\tau)}^{H} \gamma ^{t} \nabla \log \pi_{\theta}(a_{t}|s_{t})\hat{A}(s_{t},a_{t})  \right]
  \end{equation}

  \paragraph{Marginalised Importance Sampling} Remember that \(d^{\pi}(s)\) is a distribution over state visitation frequencies.

  The \emph{state-marginal importance ratio} \(\rho^{\pi_{\theta}}= \frac{d^{\pi_{\theta}}(s)}{d^{\pi_{\beta}}(s)} \)


  I didn't fully understand this part and didn't have time to look into it.

\end{document}
