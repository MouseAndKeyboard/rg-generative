\documentclass{article}
\title{Notes for: Attention Is All You Need}
\author{Michael N.\thanks{paper: https://arxiv.org/abs/1706.03762}}
\date{\today}
\usepackage{graphicx}


\begin{document}
    \maketitle
    
    \section{High Level and Motivation}
    
    \paragraph{Firstly} As the name suggests, the paper deals with "transformation" tasks such as, among other things, language models (think gpt-2) and machine translation.

    \paragraph{RNNs, LSTMs and GRUs} suffer because they are not parallelizable, struggle when they need to maintain long-distance relationships and some of their use-cases in seq-to-seq could be seen as \emph{immoral} (*gasp!*). Are there better solutions?

    \subsection{}


    \section{Core content}

    \section{Their Experiments and Comments}

\end{document}