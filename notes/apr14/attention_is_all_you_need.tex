\documentclass{article}
\title{Notes for: Attention Is All You Need}
\author{Michael N.\thanks{paper: https://arxiv.org/abs/1706.03762}}
\date{\today}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{ulem}

\begin{document}
    \maketitle
    
    \section{High Level and Motivation}
    
    \paragraph{Firstly} As the name suggests, the paper deals with "transformation" tasks such as, among other things, language models (think gpt-2) and machine translation.

    \subsection{Limitations of previous methods}
    \paragraph{RNNs, LSTMs and GRUs} suffer because they are not parallelizable, struggle when they need to maintain long-distance relationships and some of their use-cases in seq-to-seq could be seen as \emph{immoral} (*gasp!*). Are there better solutions?

    \subsubsection{Parallelizable}
    \paragraph{Recurrent models} tend to operate by taking the history of previously generated tokens and predicting the next one conditioned on that history:
    $$p(t_n|t_{n-1}, t_{n-2}, \cdots, t_1, t_0)$$

    \paragraph{} By this definition we can see that if you want to predict 100 tokens, e.g. 100 words, you'll have to follow this sequential process: 
    \begin{enumerate}
        \item predict the first token
        \item predict the next conditioned on the first
        \item predict the next conditioned on the previous two
        \item etc x100.
    \end{enumerate}

    \paragraph{If you wanted to generate} a massive $n$ tokens it will take you $O(n)$ time, and there's no way to parallelize this operation.

    \subsubsection{Long distance relationships}
    
    \paragraph{The vanishing gradient problem} is the problem with RNNs and to a lesser extent LSTMs and GRUs that inputs from many time-steps ago become "diluted" and "lost" amongst newer inputs.
    \paragraph{} This is a concerning issue with NLP and many seq-to-seq tasks in general because, using language as an example, contextual information such as the conversation topic may only be mentioned once at an early time in a sentence. For example, consider this current document, an RNN would have a hard time storing the information that the current topic is the "Limitations of previous methods" because it happened very early on.

    \paragraph{Recurrent Neural Netork}

    $$h_t = f_\theta(h_{t-1}, x_t)$$
    $$h_t = f_\theta(f_\theta(h_{t-2}, x_{t-1}), x_t)$$
    
        As $x_t$, $x_{t-1}$, $\cdots$ are combined, it's likely that unless $\theta$ is configured perfectly there will be a larger likelyhood of $t_0$'s information being "lost".

    \subsubsection{Immorality}

    \paragraph{Is position that important?} RNNs, LSTMs, GRUs and even non-recurrent methods such as Wavenet (which uses CNNs) all capture the spacial relationships of tokens well. However, in problems such as NLP, they don't capture the highly interconnectedness of words. Take for example Winograd schemas...

    \paragraph{Winograd Schema Challenge} Winograd Schema questions simply require the machine to identify the antecedent of an ambiguous pronoun in a statement. 

    \begin{itemize}
        \item The animal didn't cross the street because \emph{it} was too tired.
        \item The animal didn't cross the street because \emph{it} was too wide.
    \end{itemize}

    What is \emph{it}?
    All of the techniques discussed so far fail pitifully (or are not great) at these tasks

    \paragraph{What's needed?} Perhaps some mechanism which relates tokens between each other directly...

    \includegraphics[angle=-90,origin=c,scale=0.1]{fig1.png}

    \subsection{Introducing attention}
    \subsubsection{Intuition of attention}

    \includegraphics[scale=0.16]{fig2.png}

    \paragraph{} As shown in this figure, each output node "attends" to \emph{Values} (the lower row of nodes) and essentially is the weighted sum of that row.  

    \subsubsection{Dot product attention}
    This is the vanilla way to do attention.

    \paragraph{There are 3 inputs into an attention algorithim}
    \begin{enumerate}
        \item[Key $K$] $n$ vectors* of dimension $d_K$ 
        \item[Value $V$] $n$ vectors* of dimension $d_V$ 
        \item[Query $Q$] Vector of dimension $d_Q \equiv d_K$
    \end{enumerate}
    *These are matrices in reality.

    \paragraph{What do we do?} 
    \begin{enumerate}
        \item Each Key is associated with a Value. (think: dictionary)
        \item We need to find the similarity between our Query and each Key
        \item Multiply each Value by the corresponding similarity which we just calculated between the Query and Key
        \item Each weighted Value is then summed.
        \item Output the result
    \end{enumerate}

    \paragraph{Dot product reminder} The dot-product between two vectors: 
    $$\vec{A} \cdot \vec{B} = |A||B|\cos{\theta}$$
    
    When the two vectors are orthogonal $\cos(\frac{\pi}{2}) = 0$ and therefore the total goes to 0.
    When they are similar (smaller angle between them) $\cos(0) = 1$ and the resulting product will be higher.

    \paragraph{How does this relate to attention?} Remember that we want to compute the similarity between the single Query and the Key matrix. Based on what was just discussed, we can compute the dot product. We must also divide by a regularizer to account for large magnitudes at higher dimensions.

    \[ \frac{QK^T}{\sqrt{d_K}} \]

    \paragraph{Adding Values} Now that we have the similarity metric between the query and keys we apply the softmax function over the resulting vector and multiply the result by the Value matrix.

    \[ A(Q,K,V) = \text{softmax} \left ( \frac{QK^T}{\sqrt{d_K}} \right) V \]

    \begin{center}
        \includegraphics[scale=0.5]{fig3.png}
    \end{center}

    \subsection{What is a transformer?}
    \paragraph{} \sout{All this talk about attention might be making you lose attention}
    \paragraph{} A transformer is very simple conceptually (the name gives it away). It simply maps a sequence of inputs $(x_1, \dots, x_n)$ to a sequence of representations $(z_1, \dots, z_n)$. Given $z$, the transformer then generates an output sequence $(y_1, \dots, y_m)$.
    Example: machine translation, you start with English sentences, these sentences are mapped to a representation, then the transformer generates the desired language based on the representation.

    \section{Core content}

    \subsection{Architecture}
    \includegraphics[scale=0.3]{fig4.png}

    The architecture is made up of different components which can be explained at different levels.
    \pagebreak
    \begin{enumerate}
        \item Inputs
        \begin{itemize}
            \item Word embeddings
            \item Encoder inputs
            \item Decoder inputs (labelled Outputs)
            \item Positional encodings
        \end{itemize}
        \item Encoder
        \begin{itemize}
            \item Multi-head self-attention mechanism
            \item Position-wise fully connected feed forward network
            \item Layer normalization
        \end{itemize}
        \item Decoder
        \begin{itemize}
            \item \emph{Masked} Multi-head self-attention mechanism
            \item Multi-head attention over encoder outputs
            \item Position-wise fully connected feed forward network
            \item Layer normalization
        \end{itemize}
        \item Outputs
        \begin{itemize}
            \item Linear transformation
            \item Softmax operation
        \end{itemize}
    \end{enumerate}
    \subsection{Inputs}

    \subsubsection{"Inputs"}
    \paragraph{} The literal sequence of tokens which will be processed. For MT this might be an english sentence, e.g. "Hello World"

    \subsubsection{Embeddings}
    \paragraph{} Words are embedded using some method (not really important). E.g. the two words "Hello World" my be encoded in the following two vectors (details not necessary).
    \[ \text{Hello} = \begin{bmatrix}
        0 & 0.2 & 0.8 & 0.3
    \end{bmatrix} \]
    \[ \text{World} = \begin{bmatrix}
        0.4 & 0.4 & 0.1 & 0.6
    \end{bmatrix} \]

    \subsubsection{Positional encodings}
    \paragraph{} Because attention networks don't "see" position and therefore the distance between tokens, we need to explicitly add the token's position information.
    \paragraph{} They do this by encoding position using a number of sine waves and then finding the values of each sine wave evaluated at the token's position.
    For example: the token at position 1 might have a position embedding which looks like:
    \[ \begin{bmatrix}
        0.84 & 0.33 & 0.2 & 0.14
    \end{bmatrix} \]

    \paragraph{} From what I can tell, this value is added elementwise with the word embedding.

    Example of sine curves (values are not true to form)

    \includegraphics[width=10.0cm]{fig5.png}

    \subsection{Encoder}
    \subsubsection{What does the encoder do?}
    It converts some input sequence into a hidden representation 

    \subsection{Self-Attention}
    \subsubsection{What is it?}

    \paragraph{} In the architecture, self-attention is used, inputs seemingly are converted into Keys, Values \textbf{and} Queries. What does this mean!?
    \paragraph{The answer is quite simple really...} The input matrix $X$ is multiplied by 3 weight matrices:
    \[ X \times W^Q = Q \]
    \[ X \times W^K = K \]
    \[ X \times W^V = V \]

    These weight matrices are parameters to be learned by the network.

    \subsubsection{Intuition?}
    You generate an output of values which are some hidden representation of the input X which (after training) better codes for relationships between elements as the output "attends" to the whole input $X$.

    \subsection{Multi-Head Self-Attention}

    \begin{center}
        \includegraphics[scale=0.2]{fig6.png}
    \end{center}

    \subsubsection{Purpose}

    In regular attention we softmax over the weighted average of our result vectors. This process is lossy because...
    %TODO: Write purpose% 

    \begin{quote}
        Multi-headed attention allows the model to jointly attend to information from different representation subspaces at different positions.
    \end{quote} 

    \subsubsection{How it's made}
    We linearly project our input, $X$, $h$ times (for queries, keys and values) with different parameter matrices each time.

    \[ X_i \times W_i^K = K_i\]
    \[ X_i \times W_i^Q = Q_i \]
    \[ X_i \times W_i^V = V_i \]

    \paragraph{From the paper} Instead of performing a single attention function with
    \linebreak
    $d_{model}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$, $d_k$ and $d_v$ dimensions respectively.

    \section{Their Experiments and Comments}

\end{document}